# Todos — Pending Repairs

Issues confirmed via log analysis and architectural review (2026-02-20).
Each item references its issue number for full root-cause detail.

---

## P0 — Architecture-Breaking (fix before anything else)

### T1 — Strip subtask IDs from planner output; assign in Go (Issue #27)
The planner LLM fabricates fake UUIDs. The top repeated ID appears 270 times,
breaking all dispatcher routing guarantees.
- Remove `subtask_id` from the planner JSON schema and system prompt
- After parsing planner output, assign `uuid.NewString()` to each subtask in Go
- Verify no subtask ID is ever generated by the LLM

### T2 — Enforce R4b hard-fail when any subtask failed (Issue #28)
R4b currently accepts 1-matched + 1-failed = task accepted. This delivers wrong results
silently.
- In Go code, before calling the R4b LLM: if any `SubTaskOutcome.Status == "failed"`,
  emit `replan` immediately without LLM involvement
- LLM is only invoked to merge outputs from subtasks that all passed

---

## P1 — Correctness (system produces wrong answers)

### T3 — Per-criterion validation in R4a and R4b (Issue #30)
Validators reason holistically; a plausible-sounding result carries even when specific
criteria are unmet.
- R4a: restructure prompt to evaluate each `success_criterion` independently, emit
  explicit `pass`/`fail` per criterion; require ALL to pass for `matched`
- R4b: enforce in code that all subtask outcomes are `matched` before calling LLM
  (see T2); LLM merge step only assembles output, does not re-adjudicate pass/fail

### T4 — Connect R1 (Perceiver) to memory (Issue #29, Layer A)
Misunderstandings are locked in at R1 before R2 ever queries memory. R1 is
completely memory-blind.
- R1 must send `MsgMemoryRead` for ambiguous or repeated tasks before publishing
  `TaskSpec`
- On memory hit: inject relevant episodic/procedural entries into the Perceiver
  prompt so it can correct its interpretation before characterising the task

### T5 — Enforce memory use in R2 planner (Issue #29, Layer B)
R2 queries memory but the results are advisory — the LLM ignores them.
- If `MsgMemoryResponse` contains procedural entries (failure lessons) matching the
  current task, add a code-level check that the new plan differs from the recorded
  failed approach (different tool choice, different subtask structure, etc.)
- At minimum: include memory entries in the planner prompt under a clearly marked
  section with instruction "you MUST avoid these previously failed approaches"

---

## P2 — Reliability (system throws away correct results)

### T6 — Strip trailing prose from LLM JSON output (Issue #31)
Trailing prose after `}` causes `json.Unmarshal` to fail, discarding correct
executions as infrastructure errors.
- After `StripFences()`, find the last `}` that closes the top-level object and
  truncate there before unmarshalling
- Add a test: JSON with trailing prose must parse correctly

### T7 — Fix `redirectPersonalFind` pattern extraction and mdfind glob handling (Issue #32)
Two failures: `-path` filter is silently dropped; mdfind receives glob patterns it
cannot match.
- Extract only the bare stem from the `-name` value (strip `*`, extensions if needed)
  before passing to `mdfind`
- If `-path` is present in the original command, do not redirect to mdfind; pass
  through to shell `find` with the path scoped to the specific subdirectory
- Add tests for both cases

---

## P3 — Observability (silent failures)

### T8 — Search tool timeout: log and surface to LLM explicitly
DuckDuckGo timeouts are silent; the executor proceeds and fabricates answers.
- On timeout, return an explicit error string to the LLM: "search unavailable —
  answer only from tool output already collected; do not guess"
- Consider a shorter timeout (5s) with a retry before giving up
