# Bug Log & Fix History
[toc]

Bugs discovered and fixed during the first end-to-end test session (2026-02-19).

---

## Issue #1 ‚Äî Executor returns wrong result: "No .go files found"

**Symptom**
Running `agsh "list all go files in the current directory"` returned:
> "No .go files found in the current working directory, which satisfies all the task's success
> criteria (all .go files included, no non-.go files present‚Ä¶)"

The project clearly has 17 `.go` files. The answer was factually wrong.

**Root causes**
1. The executor had no working-directory context in its prompt. The LLM didn't know where it was running.
2. The LLM model (DeepSeek/Volcengine) consistently ignores system-prompt instructions and emits
   `find . -maxdepth 1 -name '*.go'` ‚Äî which finds nothing because all project files live in
   subdirectories, not the repo root.
3. The AgentValidator system prompt was too permissive: it accepted an empty/negative result as
   "matched" without requiring positive evidence.

**Fix attempts (in order)**

| # | What was tried | Outcome |
|---|---|---|
| 1 | Add `os.Getwd()` to executor user prompt so LLM knows current dir | LLM still used `-maxdepth 1`, still found nothing |
| 2 | Strengthen executor system prompt: "use `find . -name '*.go'` without `-maxdepth`" | Model ignored the instruction |
| 3 | Strengthen AgentValidator prompt: require positive evidence for "matched" | Validator now sends corrections, but executor repeated the same wrong command |
| 4 | Pass tool-call history across correction rounds so executor can't repeat itself | Executor tried different wrong commands (`ls *.go`, `ls -la`) but still no recursive find |
| 5 | Add `normalizeFindCmd()` in `runTool` to strip `-maxdepth N` from all find commands before execution | **Fixed.** Deterministic code guardrail bypasses model non-compliance |
| 6 | Add `glob` tool (Go `filepath.WalkDir`, always recursive) and make it the preferred tool for file discovery | **Better fix.** Model naturally picks `glob` for file tasks; no shell subprocess, no depth issue |

**Root fix**: Code-level normalisation + a dedicated `glob` tool. Prompt engineering alone is insufficient when the underlying model reliably ignores specific instructions.

---

## Issue #2 ‚Äî Tasks are too slow (5‚Äì7 minutes for a trivial query)

**Symptom**
Counting or listing files took 5‚Äì7 minutes. Every simple task generated ~18 LLM calls.

**Root causes**
1. Planner decomposed simple single-step tasks (e.g. "list files") into 2 subtasks, doubling cost.
2. Executor returned `status: "uncertain"` after every tool call instead of committing to `"completed"`,
   triggering AgentValidator correction loops.
3. Correction rounds started fresh (`execute()` was called anew each time), so the executor had no
   memory of what it had already tried and repeated the same wrong command.
4. `maxRetries = 3` allowed up to 4 LLM calls per subtask before failing.

**Fix attempts (in order)**

| # | What was tried | Outcome |
|---|---|---|
| 1 | Update planner prompt: "prefer a single SubTask for simple operations" | Planner now emits 1 subtask for most simple queries |
| 2 | Update executor prompt: "use `completed` after seeing tool output; `uncertain` only when genuinely ambiguous" | Reduced but did not eliminate `uncertain` loops |
| 3 | After tool results, append "You have the output. Output the final result now." to the prompt | Executor committed faster |
| 4 | Pass accumulated tool-call history into correction rounds | Avoids repeating identical commands across retries |
| 5 | Reduce `maxRetries` from 3 ‚Üí 2 | Saves one correction round per subtask on failure |
| 6 | Add debug logging of each LLM response and tool call | Exposed the `-maxdepth 1` root cause for Issue #1 |

**Root fix**: Issues #1 and #2 are coupled ‚Äî once the executor used `glob` and got correct output on the first try, the correction loop disappeared entirely. Typical simple task now takes ~20‚Äì25 s (perceiver + planner + one executor tool call + agentval + metaval).

---

## Issue #3 ‚Äî Memory not written for failed tasks

**Symptom**
After repeated failures on "list go files", the memory store had no record of what went wrong.
On the next session the system made the exact same mistakes.

**Root cause**
`MetaValidator.evaluate()` only wrote a `MemoryEntry` on `verdict == "accept"`. The `"replan"` branch
wrote nothing. Failed tasks left no institutional knowledge.

**Fix**
Added a `"procedural"` memory entry in the `"replan"` branch of `evaluate()`:
```json
{
  "type": "procedural",
  "content": {
    "lesson": "Task failed: <gap_summary>. Avoid repeating the same approach.",
    "gap_summary": "...",
    "failed_subtasks": ["..."]
  },
  "tags": ["failure", "replan", "<task_id>", "<keywords from gap_summary>"]
}
```
This gives the Planner context on what failed and why before it replans.

---

## Issue #4 ‚Äî `MemoryQuery.Query` field silently ignored

**Symptom**
The Planner queried memory with `MemoryQuery{Query: spec.Intent}` (natural language), but the
memory store never used this field. All memory reads returned either everything or nothing,
regardless of the query.

**Root cause**
`Store.Query(taskID, tags string)` filtered by exact `taskID` match and substring `tags` match only.
The `Query` (natural-language) field of `MemoryQuery` was accepted in the struct but never passed
to `Store.Query()`.

**Fix**
- Added `query string` parameter to `Store.Query()`.
- Tokenises the query into words (‚â•3 chars), serialises each candidate entry to JSON, and checks
  whether any keyword appears in the serialised text.
- Updated `Run` handler to pass `query.Query` through.

---

## Issue #5 ‚Äî Memory entry lost on one-shot exit

**Symptom**
In one-shot mode (`agsh "some task"`), the success `MemoryEntry` published by MetaValidator was
never persisted to `memory.json`. Subsequent sessions had no record of completed tasks.

**Root cause**
Race condition on shutdown: `MetaValidator` publishes `MsgMemoryWrite` to the bus, then calls
`outputFn()` which unblocks `runTask()`, which returns, causing `main()` to return and the process
to exit. The memory store's goroutine never got CPU time to dequeue and process the message.

**Fix attempts (in order)**

| # | What was tried | Outcome |
|---|---|---|
| 1 | Add `cancel()` call after `runTask` returns in one-shot mode | Memory goroutine received ctx.Done() but exited immediately without draining |
| 2 | Drain pending writes in `memory.Run` on `ctx.Done()` (select-default loop) | Entries now dequeued, but process had already exited |
| 3 | Both: `cancel()` + drain in Run + `time.Sleep(200ms)` in main | **Fixed.** 200 ms gives the scheduler time to run the memory goroutine's drain loop |

**Root fix**: Graceful shutdown ‚Äî `memory.Run` drains its write channel on context cancellation, and the main goroutine waits briefly before exiting.

---

## Issue #6 ‚Äî Memory tags are useless for retrieval

**Symptom**
All memory entries had tags `["task", "<taskID>"]`. The Planner could only retrieve entries for
the exact same `taskID`, making cross-task learning impossible (e.g. a lesson learned from
`list_go_files` could never be found when planning `count_go_lines`).

**Root cause**
Tags were hardcoded in `metaval.go` as `[]string{"task", taskID}` regardless of content.

**Fix**
Tags now extracted from the task `intent` field: split on whitespace, lowercase, strip punctuation,
keep words ‚â•4 chars. The `"accept"` entry gets `["success", taskID, <intent keywords>...]` and the
`"replan"` entry gets `["failure", "replan", taskID, <gap_summary keywords>...]`.

---

## Issue #7 ‚Äî REPL has no session context; follow-up inputs fail

**Symptom** (screenshot)
After the program returned a wrong line count, the user typed "bushit". The Perceiver responded:
> "I'm not sure what you're asking for. Could you please rephrase or clarify your request?"

It had no knowledge of the previous turn and treated "bushit" as a standalone, meaningless command.

**Root cause**
Each REPL turn called `perceiver.New(...).Process(ctx, input)` with only the raw new input.
No session history was maintained or passed.

**Fix**
- `runREPL` maintains a `[]sessionEntry` (last 5 turns) recording each input and its result summary.
- `buildSessionContext()` formats history as numbered turns and passes it to `perceiver.Process()`.
- `Process()` now accepts a `sessionContext string` and prepends it to the LLM user prompt.
- Perceiver system prompt updated with explicit session history rules:
  - Reactions ("wrong", "bullshit", "no") ‚Üí redo previous task with better criteria
  - Pronouns ("it", "that") ‚Üí resolve to most recent task
  - Short reactive inputs ‚Üí **never** trigger a clarification question

**Verified**
```
agsh> count go lines for this project
--- Result ---
Total lines of Go code: 1204   ‚Üê wrong

agsh> bullshit, use wc -l to count properly
--- Result ---
Total line count: 2581          ‚Üê correct; reused prior context
```

---

## Issue #8 ‚Äî REPL UX: raw log spam, no visual pipeline feedback

**Symptom**
All `[R1]`, `[R3]` debug log lines printed to the same terminal as user output, making results hard to read. No visual indication of what the system was doing while processing a query.

**Root cause**
`log.Printf` defaulted to `os.Stderr` (visible in terminal). No progress indicator. No inter-role flow visualization.

**Fix**
1. Redirect `log` output to `~/.cache/agsh/debug.log` at startup ‚Äî terminal stays clean.
2. Added `internal/ui/display.go` ‚Äî a sci-fi terminal overlay driven by a bus tap:
   - Braille spinner (`‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è`) with live status label, updated every 80 ms
   - Flow line per bus message: `  üß† R1 ‚îÄ‚îÄ[TaskSpec]‚îÄ‚îÄ‚ñ∫ üìê R2`
   - Pipeline box borders: `‚îå‚îÄ‚îÄ‚îÄ ‚ö° agsh pipeline ‚îÄ‚îÄ‚îÄ‚îÄ` / `‚îî‚îÄ‚îÄ‚îÄ ‚úÖ  18ms ‚îÄ‚îÄ‚îÄ‚îÄ`
   - Infrastructure messages (memory read/write) rendered dim; correction signals in red
   - No external dependencies ‚Äî pure ANSI escape codes
3. Modified `bus.go`: single `tapCh` replaced by `taps []chan Message`; `NewTap()` lets auditor and UI each register an independent fan-out channel.
4. REPL prompt updated to `‚ö° agsh` header + cyan `‚ùØ` prompt char.

---

## Issue #9 ‚Äî Infinite replan loop on macOS file-search tasks

**Symptom** (screenshot)
Searching for movie/music files looped: R4b kept sending ReplanRequest despite the executor finding the correct answer on the first try.

**Root causes** (from debug log analysis)
1. **macOS TCC/SIP protection**: `~/Music/Music` is system-protected. `find ~/Music` exits with status 1 (`Operation not permitted`), even though files in other directories are returned in stdout. The executor's result was correct and complete, but stderr showed a permission error.
2. **R4a "empty = incomplete" rule was too broad**: The rule "never accept empty result for a listing task" caused R4a to reject the music subtask even when the shell actually ran and found nothing ‚Äî because there genuinely are no music files in accessible directories.
3. **R4a penalised inaccessible OS directories**: It required the search to cover `~/Music` fully, which is impossible. This sent a CorrectionSignal to an executor that had already done everything it could.

**Fix**
- **`agentval.go` system prompt**: Added two explicit rules:
  - *Empty-result rule*: if `tool_calls` shows a real search was run and stdout is empty, output `matched` ‚Äî empty is a valid answer.
  - *OS permission rule*: `"Operation not permitted"` / `"Permission denied"` in stderr is an OS constraint, not an executor gap; accept the result if all accessible directories were searched.
- **`executor.go` system prompt**: Added macOS guidance ‚Äî always append `2>/dev/null` to find; never include `~/Music/Music` or `~/Library`.
- **`executor.go` `normalizeFindCmd()`**: Code-level guardrail ‚Äî automatically appends `2>/dev/null` to any `find` command that doesn't already have it, so permission errors never cause exit status 1 or hide stdout results.

---

## Issue #10 ‚Äî REPL input: backspace broken, arrow keys show codes, Chinese unsupported

**Symptom**
In the REPL:
- Backspace printed `^?` instead of deleting the previous character
- Arrow keys printed raw escape sequences (`^[[A`, `^[[B`, `^[[C`, `^[[D`)
- Up/down did not navigate command history
- Chinese (and other multi-byte Unicode) input was garbled or split across reads

**Root cause**
`bufio.Scanner` reads raw bytes from stdin with no terminal awareness.
It has no concept of terminal line editing, control sequences, or multi-byte character boundaries.

**Fix**
Replaced `bufio.Scanner` in `runREPL` with `github.com/chzyer/readline`:
- Terminal set to raw mode; readline handles backspace, `‚Üê‚Üí`, `‚Üë‚Üì` natively
- `‚Üë‚Üì` arrows navigate persistent session history (stored in `~/.cache/agsh/history`)
- Unicode-aware: correctly handles multi-byte UTF-8 including CJK wide characters
- `Ctrl+A/E` (line start/end), `Ctrl+W` (delete word) all work
- Clarify prompt uses `rl.SetPrompt()` so clarification answers also get proper editing
- `Ctrl+D` exits cleanly (EOF)
- `bufio.Scanner` retained only in `runTask` (one-shot mode, non-interactive)

---

## Issue #11 ‚Äî Ctrl+C abort: second press exits program; executor keeps running after abort

**Symptom** (screenshot)
1. First Ctrl+C: "‚ö†Ô∏è task aborted" shown correctly ‚Äî but executor/agentval goroutines kept running, display kept printing flow lines and spinning
2. Second Ctrl+C: "agsh: shutting down" ‚Äî program exited instead of returning to REPL prompt

**Root causes**
1. **Executors used main `ctx`**: `runSubtaskDispatcher` called `exec.RunSubTask(ctx, ...)` using the process-wide context. Ctrl+C cancelled only `taskCtx` (the REPL wait loop), not the goroutines doing LLM calls and tool execution.
2. **Display never saw abort**: `d.inTask` stayed `true` (no FinalResult was received), so the spinner and flow lines kept appearing after abort, flooding the terminal.
3. **Signal handler called `cancel()` when idle**: when `taskCancel == nil` (after the first abort set it to nil), the SIGINT goroutine called `cancel()` ‚Üí "agsh: shutting down". This happened in the brief window before readline re-entered raw mode (which would have intercepted Ctrl+C as `ErrInterrupt` instead of SIGINT).

**Fix**
- **Dispatcher now uses per-task contexts**: `runSubtaskDispatcher` maintains `taskCtxs map[parentTaskID ‚Üí {ctx, cancel}]`. Each executor/agentval goroutine receives the task-specific context. When Ctrl+C fires, the signal handler sends the `taskID` to `abortTaskCh`; the dispatcher calls `entry.cancel()` to stop all goroutines for that task (cancels in-flight LLM calls and shell commands immediately).
- **`Display.Abort()`**: new method sends to `abortCh`; the `Run()` goroutine calls `endTask(false)` ‚Äî prints the `‚ùå` footer, sets `inTask = false`, stops spinner and flow lines.
- **Signal handler no longer exits on second Ctrl+C**: when `taskCancel == nil` (idle), the handler does nothing. Exiting is exclusively via readline's `ErrInterrupt` ‚Üí two-press confirmation, or typing `exit`/`Ctrl+D`. This eliminates the accidental-exit race.

---

## Issue #12 ‚Äî `glob` with `root:"."` finds no user personal files

**Symptom**
```
[R3] tool glob result: (no files matched pattern *‰∏â‰∏™‰ª£Ë°®* under .)
```
Searching for a file in the user's Downloads or home directory returned nothing.

**Root causes**
1. **`root: "."` is the project working directory**, not the user's home. `GlobFiles(".", pattern)` walks `/Users/haricheung/code/agentic-shell` ‚Äî a code repository ‚Äî which contains no personal documents.
2. **`GlobFiles` did not expand `~`**: passing `root: "~"` or `root: "~/Downloads"` would walk a literal directory named `~` on disk (which doesn't exist), silently returning no results.

**Fix**
- **`internal/tools/glob.go`**: Added `~` prefix expansion at the top of `GlobFiles`:
  ```go
  if root == "~" || strings.HasPrefix(root, "~/") || strings.HasPrefix(root, "~\\") {
      home, err := os.UserHomeDir()
      if err == nil { root = home + root[1:] }
  }
  ```
- **`internal/roles/executor/executor.go` system prompt**: Added explicit `root` guidance under the `glob` tool description:
  - `root: "."` ‚Üí current project directory (code/configs in repo); use for project-scoped searches.
  - `root: "~"` ‚Üí user's home directory; use when searching for personal files (documents, downloads, music, photos, etc.).
  - `root: "~/Downloads"`, `root: "~/Documents"`, etc. ‚Üí specific user directories.
  - **Never** use `root: "."` to search for user personal files ‚Äî it will find nothing outside the project.

---

## Issue #13 ‚Äî Stale ExecutionResult published after Ctrl+C reopens pipeline box

**Symptom** (screenshot)
After aborting a task (‚ö†Ô∏è task aborted / ‚ùå box closed), a new pipeline box immediately opened at the REPL prompt showing `R3 ‚îÄ‚îÄ[ExecutionResult: failed]‚îÄ‚îÄ‚ñ∫ R4a`. The REPL was idle with no task running.

**Root causes** (confirmed via debug log)
1. When the task context is cancelled by Ctrl+C, the in-flight LLM call returns a `context canceled` error. `RunSubTask` caught the error and built a `failed` result ‚Äî then **still published it to the bus** unconditionally.
2. The display tap received the `ExecutionResult: failed` message, saw `inTask == false` (Abort had just fired), and called `startTask()`, opening a new pipeline box.
3. There was no suppression mechanism to block stale post-abort messages from triggering a new box.

**Fix**
- **`executor.go` `RunSubTask`**: Check `ctx.Err() != nil` before every `Publish()` call ‚Äî both the initial result and correction-round results. If the context is cancelled, return immediately without publishing. This stops the cascade at source.
- **`display.go`**: Added `suppressed bool` field + `Resume()` method. `Abort()` now also sets `suppressed = true`. Incoming bus messages while suppressed and `!inTask` are drained silently (no `startTask()`). Acts as a safety net for any message that was already in-flight when the executor check fires.
- **`main.go`**: `disp.Resume()` called at the top of each new REPL task (before `perceiver.Process()`), lifting the suppression exactly when the user submits a new query.

---

## Issue #14 ‚Äî Personal file search takes 6 minutes (find ~ / glob root:"~" both scan entire home)

**Symptom**
```
time find ~ -name '*‰∏â‰∏™‰ª£Ë°®*' -type f 2>/dev/null | head -20
# ‚Üí 0.46s user 7.75s system 2% cpu 5:51.35 total
```
Finding a single file by name in the home directory took nearly 6 minutes.

**Root cause**
Both `find ~` (shell) and `GlobFiles(root:"~")` enumerate every inode under `~` ‚Äî including `~/Library`, cloud sync folders, and millions of cached files ‚Äî because they have no OS index. The result is the same slow scan regardless of which tool is used.

**Fix**
Added `mdfind` as a first-class executor tool backed by macOS Spotlight:
- **`internal/tools/mdfind.go`**: `RunMdfind(ctx, query)` calls `mdfind -name <query> 2>/dev/null`. Spotlight's persistent index returns results in < 100 ms regardless of file location.
- **`executor.go` `runTool`**: new `"mdfind"` case.
- **Executor system prompt**: `mdfind` listed first with explicit ALWAYS-use guidance for personal file searches. `glob` demoted to project-only (source code, configs). Decision step updated accordingly.

**Benchmark**: `mdfind -name '‰∏â‰∏™‰ª£Ë°®'` ‚Üí **77 ms** vs `find ~` ‚Üí **351 s** (4500√ó faster).

---

## Issue #15 ‚Äî `glob` silently returns 0 results for globstar patterns (`**/*.go`)

**Symptom**
LLMs routinely emit patterns like `**/*.go` or `*/*.json` (shell globstar style). These returned 0 results with no error.

**Root cause**
`GlobFiles` matched the pattern against `d.Name()` (filename only, no path separators). Any `/` in the pattern causes `filepath.Match` to return `false` for every file. Also, the example `"pattern":"*.go"` in the executor system prompt biased the LLM toward Go-specific patterns.

**Fix**
- **`glob.go` `GlobFiles`**: strip everything up to and including the last `/` from the pattern before matching. `"**/*.go"` ‚Üí `"*.go"`, `"*/*.json"` ‚Üí `"*.json"`. Verified: `GlobFiles(".", "**/*.go")` now returns the same 8 files as `GlobFiles(".", "*.go")`.
- **Executor system prompt**: example changed from `"pattern":"*.go"` to `"pattern":"*.json"`; added note *"Pattern matches the FILENAME ONLY ‚Äî do NOT include '/'"*.

---

## Issue #16 ‚Äî Result output shows literal `\n` instead of rendered newlines

**Symptom**
```
"Available free disk space:\n- / (root): 191 GiB\n- /System/Volumes/VM: 191 GiB\n..."
```
Newlines in the output string were displayed as the two-character sequence `\n` instead of actual line breaks.

**Root cause**
`printResult` passed all output through `json.MarshalIndent`, which wraps strings in double-quotes and escapes real newlines to `\n`. The JSON-encoded string was then printed verbatim.

**Fix**
`printResult` now:
1. Marshals output to JSON, then attempts `json.Unmarshal` into a `string`.
2. If successful (plain string output): prints directly with `fmt.Println` ‚Äî real newlines render correctly.
3. If not (structured object/array): falls back to `json.MarshalIndent` for pretty JSON.
4. Suppresses the output block when it duplicates the summary.

---

## Issue #17 ‚Äî Subtask B fails because it ran in parallel with subtask A that locates its input

**Symptom** (debug log)
```
[R4a] subtask=b2c3d4e5... FAILED: Prerequisite not met: source file '‰∏â‰∏™‰ª£Ë°®.mp4' was not
      found via mdfind. The subtask cannot proceed as it depends on the successful location
      of the source file, which failed.
```
Meanwhile subtask A (locate file) succeeded and was matched. The file was present all along.

**Root cause**
The dispatcher launched ALL subtasks simultaneously, ignoring the `sequence` field. Subtask A
(sequence=1, locate file) and subtask B (sequence=2, extract audio) ran in parallel. Subtask B
had to re-discover the file itself; mdfind returned nothing for that query, so it failed.

Two compounding factors:
1. `sequence` was correctly set by the planner, but the dispatcher never read it.
2. The planner prompt only said "same sequence = parallel" ‚Äî it never said "different sequence = dependency ordering", so future plans might still assign same-sequence to dependent tasks.

**Fix**
- **`cmd/agsh/main.go` `runSubtaskDispatcher`**: Rewrote the dispatcher to implement sequential dispatch:
  - Subscribes to `MsgDispatchManifest` to learn expected subtask count.
  - Buffers incoming subtasks in `bySeq map[int][]SubTask`.
  - Once all expected subtasks arrive, dispatches the lowest sequence group.
  - Each agentval goroutine signals completion via `completionCh`; when inFlight reaches 0, the next sequence group is dispatched.
  - Outputs from completed sequences are collected and injected into every next-sequence subtask's `Context` field as "Outputs from prior steps (use these directly ‚Äî do not re-run discovery)".
- **Planner system prompt**: Added explicit sequence rules ‚Äî different sequence numbers for dependent subtasks, same sequence number for truly independent parallel subtasks. Explained that the dispatcher injects prior-step outputs automatically.

**Behaviour after fix**
```
sequence=1: locate file ‚Üí mdfind ‚Üí /Users/haricheung/Downloads/‰∏â‰∏™‰ª£Ë°®.mp4
            [wait for completion]
sequence=2: extract audio (context includes "prior step output: /Users/.../‰∏â‰∏™‰ª£Ë°®.mp4")
            ‚Üí uses the injected path directly, no re-discovery needed
```

---

## Issue #18 ‚Äî LLM hallucinates ffmpeg failure; task abandoned despite success

**Symptom**
Task "extract audio from ‰∏â‰∏™‰ª£Ë°®.mp4" was abandoned after 3 replan rounds with:
> "Task abandoned after 3 failed attempts. No new audio file was created because ffmpeg failed
> to overwrite existing file, and verification subtask could not confirm file existence or
> playability due to missing file."

The file `/Users/haricheung/Downloads/‰∏â‰∏™‰ª£Ë°®.mp3` actually existed (526 KB, valid MP3, 28s).
The ffmpeg command with `-y` flag had succeeded in replan round 2.

**Root cause**
`firstN(result, 2000)` in `executor.go` truncated shell tool output to 2000 characters before
passing it to the LLM. ffmpeg's version banner + build configuration alone is ~2500 characters,
so the LLM context window for that tool call ended mid-config-dump ‚Äî **the actual encoding
result or error line was never visible to the LLM**.

The cascade:
1. Replan 1: `ffmpeg ... ‰∏â‰∏™‰ª£Ë°®.mp3` (no `-y`) ‚Üí file already exists ‚Üí real error, correctly
   reported as failed.
2. Replan 2: `ffmpeg ... -y ... ‰∏â‰∏™‰ª£Ë°®.mp3` ‚Üí **actually succeeded** (overwrites), but LLM
   saw only the truncated banner ‚Üí hallucinated "file already exists" ‚Üí reported `status: failed`.
3. Verification subtask: `ls -la` showed the file (526249 bytes), `afplay` played it, `ffprobe`
   confirmed 28s duration ‚Äî all proving success. But R4a kept retrying because its success
   criteria included "confirm ffmpeg extraction succeeded in this run" and the extraction subtask
   was (incorrectly) marked failed. Both subtasks exhausted maxRetries=2 ‚Üí reported failed.
4. R4b saw 2/3 subtasks failed ‚Üí replanned again ‚Üí repeat √ó3 ‚Üí abandoned.

**Compounding factor**
The LLM was anchored to the previous failure ("already exists") and hallucinated the same error
even when the command succeeded, since it couldn't observe the actual result.

**Fix**
- **`executor.go`**: Replaced `firstN(result, 2000)` with `headTail(result, 4000)` for tool
  results passed to the LLM context. `headTail` preserves the first ~1333 chars AND the last
  ~2667 chars, with `...[middle truncated]...` in between. Long-banner tools like ffmpeg now
  show their actual result at the end even when the total output is large.

```go
func headTail(s string, maxLen int) string {
    if len(s) <= maxLen { return s }
    head := maxLen / 3
    tail := maxLen - head
    return s[:head] + "\n...[middle truncated]...\n" + s[len(s)-tail:]
}
```

**Verification**
ffmpeg banner is ~2500 chars; with `headTail(4000)`: head=1333 shows version+partial config,
tail=2667 shows the remaining config + actual encode output or error. The LLM now sees the result.

**Note**: This fix was necessary but not sufficient ‚Äî see Issue #19 for the R4a evidence gap
that caused the full cascade despite the executor correctly reporting `completed`.

---

## Issue #19 ‚Äî R4a rejects completed subtasks: ToolCalls carries no output evidence

**Symptom**
After Issue #18 fix, the executor LLM correctly reported `status: completed` with
`output: "MP3 file successfully created at ..."`. But R4a still scored it as `retry` both
attempts ‚Üí `max retries reached ‚Üí failed`. Task still abandoned.

**Root cause**
R4a's scoring rule: *"Trust tool output (stdout, file paths, command results) as primary evidence.
The executor's prose claim alone is not evidence."*

`ExecutionResult.ToolCalls` was populated only with tool names + command inputs:
```
["shell:ffmpeg -i '/Users/.../‰∏â‰∏™‰ª£Ë°®.mp4' -q:a 0 -map a '...mp3'"]
```
No output. R4a saw a prose claim ("MP3 file successfully created") with a tool call that had
no observable result ‚Äî exactly the pattern it's trained to distrust. Verdict: `retry`.

The actual ffmpeg output (`size= 514kB time=00:00:28.23 bitrate= 149.5kbits/s speed=19.6x`) was
only in the executor's internal `toolResultsCtx`, used to inform the LLM. It never flowed into
`ExecutionResult`.

**Fix**
- **`executor.go`**: After each `runTool` call, append the last 120 chars of actual output (or
  error string) to the corresponding `toolCallHistory` entry before it becomes `ExecutionResult.ToolCalls`:

```go
// success
toolCallHistory[len(toolCallHistory)-1] += " ‚Üí " + lastN(strings.TrimSpace(result), 120)
// error
toolCallHistory[len(toolCallHistory)-1] += " ‚Üí ERROR: " + firstN(err.Error(), 80)
```

R4a now receives:
```
"shell:ffmpeg -i '/Downloads/‰∏â‰∏™‰ª£Ë°®.mp4' -q:a 0 -map a '...mp3' ‚Üí ...
  size= 514kB time=00:00:28.23 bitrate= 149.5kbits/s speed=19.6x"
```
That is concrete, verifiable evidence ‚Üí verdict: `matched`.

**Verification**
End-to-end test: "find ‰∏â‰∏™‰ª£Ë°®.mp4 and extract its audio to mp3"
- seq=1 locate: MATCHED attempt=1 ‚úì
- seq=2 extract: MATCHED attempt=1 ‚úì (no retries)
- R4b verdict: accept ‚úì
- Output file: `/Users/haricheung/Downloads/‰∏â‰∏™‰ª£Ë°®.mp3` ‚Äî 514KB, valid MP3, 28s

---

## Issue #20 ‚Äî Spinner line-wrap floods the terminal with identical retry lines

**Symptom**
During a correction/retry, the terminal filled with dozens of identical lines:
```
‚†∏ ‚öôÔ∏è  retry 1 ‚Äî Use a macOS-compatible command like 'ps aux | sort -
‚†º ‚öôÔ∏è  retry 1 ‚Äî Use a macOS-compatible command like 'ps aux | sort -
‚†¥ ‚öôÔ∏è  retry 1 ‚Äî Use a macOS-compatible command like 'ps aux | sort -
...
```
Each line was a new spinner animation frame, not an in-place overwrite.

**Root cause**
The spinner used `\r` (carriage return) to overwrite the current line. When the status
string was long (~70 visible chars), the terminal wrapped it to a second line. `\r` then
returned the cursor to the start of the *second* (wrapped) line, not line 1. Each 80ms tick
wrote a new visible line instead of overwriting ‚Äî producing a continuous scroll of identical
frames.

The status text for corrections was built as:
`"‚öôÔ∏è  retry N ‚Äî " + clip(WhatToDo, 55)` ‚âà 70 visible chars, which wraps in an 80-col terminal.

**Fix**
- **`display.go` ticker**: changed `\r` to `\r\033[K` ‚Äî erase-to-EOL after carriage return
  clears leftover chars from longer previous statuses on the same line.
- **`display.go` `dynamicStatus`**: reduced `WhatToDo` clip from 55 ‚Üí 38 runes. Full spinner
  line is now ‚â§ 54 visible cols, safely within any terminal ‚â• 60 cols ‚Äî no wrapping possible.

---

## Issue #21 ‚Äî Model repeatedly uses `find /Users/...` instead of `mdfind`

**Symptom**
Despite the executor system prompt listing `mdfind` as tool #1 for personal file searches,
the model (Volcengine/DeepSeek) repeatedly emitted slow `find /Users/haricheung` shell
commands ‚Äî taking 6+ minutes instead of <1 second.

Example from debug log:
```
[R3] running tool=shell cmd=find /Users/haricheung -name "‰∏â‰∏™‰ª£Ë°®.mp3" -o -name "‰∏â‰∏™‰ª£Ë°®*.mp3" ...
```

**Root cause**
Model non-compliance with prompt priority. The `shell` tool description also contained
"Always append 2>/dev/null to find commands" which implicitly validated using `find` at all.
Prompt reinforcement alone proved insufficient across multiple sessions.

**Fix**
Two-layer enforcement:
1. **Prompt**: `shell` description changed to "NEVER use 'find' to locate personal files ‚Äî
   use mdfind instead", removing the implicit `find` validation.
2. **Code**: `redirectPersonalFind()` in `executor.go:runTool` ‚Äî detects `shell find` commands
   targeting personal paths (`/Users/`, ` ~`, `~/`, `/home/`, `/Volumes/`) and transparently
   redirects them to `RunMdfind()` with the extracted `-name` pattern. The model receives fast
   Spotlight results without knowing the redirect happened.

```go
if query, ok := redirectPersonalFind(tc.Command); ok {
    log.Printf("[R3] redirecting personal find to mdfind: query=%q", query)
    return tools.RunMdfind(ctx, query)
}
```

Routing rules:
- `find /Users/haricheung -name "‰∏â‰∏™‰ª£Ë°®.mp3"` ‚Üí `mdfind -name '‰∏â‰∏™‰ª£Ë°®'` (redirected)
- `find ~ -name "*.pdf"` ‚Üí `mdfind -name '*.pdf'` (redirected)
- `find . -name "*.go"` ‚Üí unchanged (project search)
- `find /tmp -name "*.log"` ‚Üí unchanged (system path)

---

## Issue #22 ‚Äî `/audit` always shows zeros on process restart

**Symptom**
After exiting and restarting agsh, `/audit` immediately showed:
```
Tasks observed:  0
Avg corrections: 0.00
Gap trends:      ‚Üëimproving=0  ‚Üístable=0  ‚Üìworsening=0
No anomalies detected.
```
even though several tasks had been run in the previous session.

**Root cause**
The auditor's window stats (`tasksObserved`, `totalCorrections`, `gapTrends`,
`boundaryViolations`, `driftAlerts`, `anomalies`, `windowStart`) were held only in
memory. On process restart they were initialised to zero values in `New()`, discarding
all accumulated data from prior runs.

**Fix**
Persist window stats to `~/.cache/agsh/audit_stats.json` and reload on startup:

- **`persistedStats` struct**: mirrors the seven window fields with JSON tags.
- **`loadStats()`**: called synchronously inside `New()` before returning. Reads
  `audit_stats.json`; silently no-ops if absent (first run). Restores all seven fields
  so the window is correct from message #1 of the new session.
- **`saveStats()`**: acquires the mutex to snapshot current window fields, then writes
  JSON to `audit_stats.json` with `os.WriteFile`. Called from the auditor goroutine only
  (no extra lock needed for the write itself).
- **`process()` call site**: `saveStats()` is called only after `MsgDispatchManifest`,
  `MsgReplanRequest`, or any anomaly ‚Äî the three event types that actually mutate stats.
  Not called on every tap message (20+ per task).
- **`publishReport()` call site**: `saveStats()` is called immediately after the window
  reset so a crash right after `/audit` doesn't replay old stats on next start.
- **`auditor.New()` signature updated**: `statsPath string` added as fourth parameter
  (before `interval`).
- **`main.go`**: passes `filepath.Join(cacheDir, "audit_stats.json")` as `statsPath`.

**Behaviour after fix**
```
Session 1:
  agsh> list all go files      ‚Üê tasks=1 recorded
  agsh> exit

Session 2:
  agsh> /audit
  Tasks observed:  1            ‚Üê restored from audit_stats.json
  window_start: 2026-02-19...
```
After `/audit` triggers a report the window resets and the zeroed stats are immediately
persisted, so restarting again shows tasks=0 with the new `window_start`.

---

## Issue #23 ‚Äî Clarification question printed dozens of times before user types anything

**Symptom**
```
‚ùØ ÁÆóÁÆóÁõÆÂâçÊàëÁî®ÁöÑÂ§ñÊé•ÊòæÁ§∫Âô®ÊòØ‰ªÄ‰πàÂ∞∫ÂØ∏ÁöÑ
? ÊÇ®ÈúÄË¶ÅÊàëÈÄöËøá‰ªÄ‰πàÊñπÂºèÊù•Á°ÆÂÆöÂ§ñÊé•ÊòæÁ§∫Âô®ÁöÑÂ∞∫ÂØ∏Ôºü...
? ÊÇ®ÈúÄË¶ÅÊàëÈÄöËøá‰ªÄ‰πàÊñπÂºèÊù•Á°ÆÂÆöÂ§ñÊé•ÊòæÁ§∫Âô®ÁöÑÂ∞∫ÂØ∏Ôºü...
... (√ó12)
‚ùØ up to you
```
The same clarification question appeared ~12 times before the user had typed any answer.

**Root causes**
1. **No cap on clarification rounds**: `perceiver.Process()` looped unconditionally ‚Äî if the
   model returned `needs_clarification: true` more than once, the loop continued forever.
2. **IME-buffered empty keystrokes**: Typing Chinese input via an IME can leave residual
   keystrokes in the terminal buffer. When `rl.Readline()` was called inside `clarifyFn`,
   it consumed those buffered empty strokes and returned `""` immediately ‚Äî without blocking
   for real user input. Each empty answer caused the model to be called again with a blank
   clarification, which returned `needs_clarification: true` again, calling `clarifyFn`
   again, and so on until the buffer was drained.

**Fix** (`perceiver/perceiver.go`):
- **`maxClarificationRounds = 2`**: `Process()` now loops at most twice before giving up.
- **Empty answer ‚Üí break**: if the user provides an empty answer (Enter with no text),
  treat it as "proceed with your best interpretation" and exit the loop immediately.
- **Final commit call**: after the loop exits (max rounds or empty answer), `perceive()` is
  called one final time with an appended instruction
  `"[Instruction: proceed with the best interpretation; do not request further clarification.]"`
  to force the model to emit a `TaskSpec` instead of another `needs_clarification`.
- **`publish()` helper**: extracted from the loop body to avoid duplicating the bus publish +
  log line.

**Behaviour after fix**
- First clarification round: model asks ‚Üí user sees question ‚Üí user types answer ‚Üí loop
- Second clarification round (if model still unclear): model asks once more ‚Üí user answers
- After two rounds, or on empty answer: model is forced to commit; task proceeds.

---

## Issue #24 ‚Äî `‚ùØ` prompt not shown after task completes

**Symptom**
After a task finishes, the REPL sometimes returned with no visible prompt ‚Äî the `‚ùØ` readline
cursor was missing. The pipeline footer (`‚îî‚îÄ‚îÄ‚îÄ ‚úÖ ...`) appeared but the prompt did not.

**Root cause**
Race condition between the display goroutine and the REPL goroutine:

1. MetaValidator calls `bus.Publish(MsgFinalResult)` ‚Äî puts tap message in display goroutine's
   channel buffer, but the goroutine may not have run yet.
2. MetaValidator calls `outputFn()` ‚Äî fills `resultCh`, unblocking the REPL goroutine.
3. REPL goroutine: `printResult()` ‚Üí breaks out of `waitResult` ‚Üí calls `rl.Readline()`,
   which draws `‚ùØ ` on the current terminal line.
4. Display goroutine: finally gets scheduled, receives `MsgFinalResult` tap ‚Üí `endTask()` ‚Üí
   prints `\r\033[K‚îî‚îÄ‚îÄ‚îÄ ‚úÖ ...`. The `\r\033[K` erases the `‚ùØ` that readline drew.

The REPL goroutine and display goroutine race to the terminal. The `‚ùØ` is erased whenever
the display goroutine loses the race and fires after readline.

**Fix**
Added `WaitTaskClose(timeout time.Duration)` to `Display` ‚Äî a synchronisation point that
blocks until the pipeline box is fully closed:

- **`taskDone chan struct{}`** field added to `Display`; created in `startTask()` under
  the mutex, closed (and nilled) by `endTask()` under the mutex.
- **`WaitTaskClose(300ms)`** called in the REPL's `waitResult` loop immediately after
  receiving the final result, **before** `printResult()` and before readline resumes.
  The 300 ms timeout prevents deadlock if the display goroutine is stuck.

Order after fix (deterministic):
1. Display: `endTask()` prints footer ‚Üí closes `taskDone`
2. REPL: `WaitTaskClose` returns ‚Üí `printResult()` ‚Üí `rl.Readline()` draws `‚ùØ`

---

## Issue #25 ‚Äî Clarification question reprinted on every readline internal redraw

**Symptom**
```
? Are you asking for the physical monitor size (like 27 inches) or...
? Are you asking for the physical monitor size (like 27 inches) or...
... (√ó20+)
```
The question line repeated dozens of times during a single `clarifyFn` call, even
after the Issue #23 loop cap was in place.

**Root cause**
The readline prompt was set with an embedded newline:
```go
rl.SetPrompt(fmt.Sprintf("\033[33m?\033[0m %s\n\033[36m‚ùØ\033[0m ", question))
```
The chzyer/readline library calculates prompt width assuming a single line. When the
prompt contains `\n`, readline only tracks the portion after the newline (`‚ùØ `) as
the "active" prompt line. On every internal redraw (terminal resize, interference from
concurrent writes, cursor movement), readline erases and redraws from the `‚ùØ` position
‚Äî but the `? question` line above it is raw output that was already scrolled into
history. Each redraw cycle prints another copy of the full two-line prompt, leaving the
previous `? question` line stranded above it. With enough redraws, dozens of copies
accumulate before the user types anything.

**Fix** (`cmd/agsh/main.go`):
Print the question with `fmt.Printf` as plain output before calling `rl.Readline()`.
The readline prompt stays as the simple single-line `‚ùØ ` which readline tracks correctly.
```go
fmt.Printf("\033[33m?\033[0m %s\n", question)
ans, err := rl.Readline()
```
The question is now printed exactly once; readline only manages its own `‚ùØ ` line.

---

## Issue #26 ‚Äî `/audit` opens a pipeline box that never closes; REPL appears stuck

**Symptom**
After running `/audit`, the pipeline box opened (`‚îå‚îÄ‚îÄ‚îÄ ‚ö° agsh pipeline`) and never
closed. The spinner kept running and the `‚ùØ` prompt was continuously erased by the 80ms
ticker's `\r\033[K`, making the REPL appear completely frozen.

**Root cause**
The display goroutine's tap handler treated `MsgAuditQuery` as a normal task pipeline
message. When `/audit` published `MsgAuditQuery`, the display saw it, called `startTask()`
(opening the pipeline box and setting `inTask = true`), and started the spinner. The
corresponding response `MsgAuditReport` was published later, but the display's `endTask()`
is only triggered by `MsgFinalResult` ‚Äî `MsgAuditReport` never matched. So `inTask` stayed
`true` indefinitely; the spinner kept firing every 80ms and erasing whatever was on the
current terminal line, including the readline prompt.

**Fix** (`internal/ui/display.go`):
Skip both `MsgAuditQuery` and `MsgAuditReport` at the top of the tap handler before any
`startTask()` logic. Audit messages are meta-system events, not task pipeline events; they
must never open a pipeline box.
```go
if msg.Type == types.MsgAuditQuery || msg.Type == types.MsgAuditReport {
    continue
}
```

---

## Issue #27 ‚Äî Planner generates fake, repeated subtask UUIDs

**Symptom**
The same subtask ID `a1b2c3d4-e5f6-7890-abcd-ef1234567890` appears 270 times across
completely different tasks in the debug log. The top repeated IDs share an obvious
sequential template pattern:
```
a1b2c3d4-e5f6-7890-abcd-ef1234567890  ‚Üí 270 uses
a1b2c3d4-e5f6-7890-abcd-ef1234567891  ‚Üí  33 uses   ‚Üê last digit incremented
a1b2c3d4-e5f6-7890-abcd-ef1234567892  ‚Üí  33 uses
e3b0c442-98fc-1c14-9afc-39c7c5d6f0b1  ‚Üí 166 uses
b2c3d4e5-f6a7-8901-bcde-f01234567890  ‚Üí 112 uses
```
The top 10 repeated IDs account for ~26% of all subtask dispatches.

**Root cause**
The planner LLM is fabricating UUIDs in its JSON output rather than delegating ID
generation to the Go runtime. The planner system prompt tells R2 to assign a `subtask_id`
to each subtask ‚Äî the LLM invents plausible-looking but deterministic UUIDs from its
training distribution. It is never called `uuid.NewString()`; it just writes hex that
looks like a UUID.

**Impact**
The entire dispatcher routing guarantee depends on subtask ID uniqueness. Results from
task A's executor targeting subtask_id X are delivered to task B's agentval goroutine if
task B reuses ID X. This is confirmed by:
```
[DISPATCHER] WARNING: no state for subtask=a1b2c3d4... (already completed?)
```
The dispatcher receives an `ExecutionResult` for an ID whose state was already cleaned
up by a different task. The result is silently dropped; the intended recipient never
receives it.

**Fix needed**
Subtask IDs must be assigned by the Go runtime after the LLM responds, not by the LLM
itself. The planner prompt should not ask for `subtask_id` in the JSON output; Go code
should call `uuid.NewString()` and inject IDs before publishing.

---

## Issue #28 ‚Äî R4b accepts tasks with failed subtasks (1 matched + 1 failed = accept)

**Symptom** (debug log)
```
[R4b] outcome for subtask=d9e0f1... status=matched (1/2)
[R4b] outcome for subtask=c8f9a0... status=failed  (2/2)
[R4b] task=linus_torvalds_recent_activity ACCEPTED
```
R4b accepted a task where one of two subtasks explicitly failed.

**Root cause**
R4b validation is LLM-based and holistic. The LLM receives all outcomes together and
reasons about whether "the overall goal" is met ‚Äî it does not mechanically check whether
every subtask passed. When a failed subtask's counterpart produced plausible-sounding
output, the LLM concluded the combined result was "good enough" and emitted `accept`.
The prompt instruction "accept only when ALL success_criteria met" is advisory to the
LLM, not an enforced code constraint.

**Impact**
The user receives a partial or wrong result and is told the task succeeded. Incorrect
answers are delivered silently with no signal that part of the execution failed.

**Fix needed**
R4b must reject (or replan) if any subtask status is `failed`, enforced in Go code
before the LLM is called. The LLM should only be invoked to merge outputs from subtasks
that all passed.

---

## Issue #29 ‚Äî Memory system is structurally bypassed; effectively write-only

**Symptom**
The same user question about monitor size was answered incorrectly three times in a row.
R5 stored failure lessons after each attempt. The agents made identical mistakes on every
retry with no evidence that prior lessons influenced any plan.

**Root cause ‚Äî two structural failures:**

1. **R1 (Perceiver) is completely memory-blind.** Misunderstandings are locked in at
   R1 before R2 ever queries memory. R1 produces a `TaskSpec` with the wrong `intent`
   and `success_criteria`, and R2 plans against that wrong spec. Any memory R2 retrieves
   is irrelevant because the task is already mis-characterised. R1 has no integration
   with R5 at all.

2. **R2 memory results are advisory, not enforced.** R2 sends `MsgMemoryRead`, receives
   entries, but its prompt gives memory the same weight as any other context ‚Äî the LLM
   may or may not act on it. The log confirms R2 dispatches identical subtask structures
   with identical fake UUIDs regardless of stored lessons. There is no code-level check
   that memory results must materially change the plan when relevant entries exist.

**Impact**
The memory system correctly stores episodic and procedural entries. Nothing reads them
in a way that changes agent behaviour. It is real infrastructure connected to nothing
that matters. The system cannot learn across sessions.

**Fix needed**
R1 must query memory before characterising ambiguous tasks. R2 must have a code-enforced
contract: if relevant procedural entries exist, the plan must demonstrate it has avoided
the previously recorded failure approach.

---

## Issue #30 ‚Äî Validators evaluate holistically; per-criterion enforcement is absent

**Symptom**
R4a accepts `status=completed, output="27 inches"` even when the tool stdout is empty ‚Äî
the claim is plausible so the LLM scores it as `matched`. R4b accepts 1-matched + 1-failed
outcomes (see Issue #28). Validators behave as lenient reviewers rather than strict
checkers.

**Root cause**
Both R4a and R4b are LLM-based validators scoring LLM-produced output. LLMs reason by
plausibility and analogy ‚Äî they are constitutionally unsuited to strict boolean
per-criterion checking. The design requires each `success_criteria` entry to be
independently verified (pass/fail), but the LLM receives all criteria together with all
output and forms a holistic impression. One plausible-sounding criterion can carry the
whole result even when others are clearly unmet.

This is the same failure as writing expectations without tests: the expectations exist in
the prompt, but without mechanical per-criterion checking they are decoration.

**Impact**
- R4a: spurious `matched` verdicts on empty or hallucinated tool output
- R4b: partial task acceptance (failed subtasks ignored if others look good)
- Correction loops that should fire do not; the system over-reports success

**Fix needed**
Validation must be restructured so that each `success_criterion` is evaluated
independently and produces an explicit boolean verdict before any holistic merge. A
single `false` must hard-fail the validation in code, regardless of what the LLM
concludes about the overall result.

---

## Issue #31 ‚Äî LLM output parser fails on trailing prose after JSON

**Symptom**
```
[R3] ERROR re-executing subtask ...: parse LLM output: invalid character 'I' after
top-level value (raw: {"action":"result",...}
I have the required output from the previous tool call...)
```
Correct executions are thrown away and republished as `status=failed`.

**Root cause**
The LLM occasionally appends explanatory prose after the closing `}` of its JSON
response. `StripFences()` strips markdown code fences but does not strip trailing
non-JSON text. `json.Unmarshal` fails on the first non-whitespace character after the
top-level value. R3 catches the parse error, and because it cannot interpret the result,
publishes a `failed` ExecutionResult. R4a then immediately fails the subtask as an
infrastructure error.

**Fix needed**
After `StripFences()`, truncate the string at the first `}` that closes the top-level
JSON object before attempting `json.Unmarshal`.

---

## Issue #32 ‚Äî `redirectPersonalFind` discards `-path` filter; mdfind rejects glob patterns

**Symptom**
```
find /Users/haricheung -type f -name "*.json" -path "*memory*"
‚Üí redirecting to mdfind: query="*.json"
‚Üí (no files found with name matching "*.json")
```
Both the path filter and the result are lost.

**Root cause ‚Äî two compounding failures:**

1. `redirectPersonalFind` extracts only the `-name` value and silently discards all
   other `find` flags including `-path`, `-type`, and `-maxdepth`. The memory file path
   constraint is lost.

2. `mdfind -name '*.json'` performs exact name matching ‚Äî Spotlight does not expand
   glob wildcards in `-name` queries. A pattern containing `*` will never match any
   file. The redirect produces guaranteed empty results for any pattern with a wildcard.

**Fix needed**
For redirected queries, extract the stem from the `-name` pattern (strip `*` and
leading path separators) and pass only that to `mdfind`. Post-filter results by
extension in Go code if needed. Alternatively, when `-path` is present, consider
using `shell find` within the specific subdirectory rather than redirecting to mdfind.

---

## Issue #33 ‚Äî Memory entries treated as advisory; calibration not code-enforced

**Symptom**
R2 queried R5 before planning and received relevant memory entries, but the LLM
planner consistently ignored them. The same failing approaches (e.g. `shell find`)
were repeated even when procedural entries explicitly documented them as failures.
Memory was effectively write-only: read path worked, but entries had no behavioral
effect on planner output.

**Root cause**
`plan()` serialised the raw `MemoryResponse` as a JSON block and appended it to the
user prompt as advisory text:
```
Prior memory entries (learn from these):
[{"type":"procedural","timestamp":"...","content":"..."}]
```
LLMs treat advisory context as optional. The model had no strong structural cue to
treat the entries as hard constraints vs. background information, so it ignored them.
The problem is Issue #29 / T5 ‚Äî memory was bypassed at the calibration step.

**Fix**
Implemented `calibrate()` in Go code (Steps 1‚Äì3 of the Memory Calibration Protocol):
- **Step 1**: Retrieve entries from R5 (already done via bus before this fix).
- **Step 2**: Sort by recency (newest first), cap at `maxMemoryEntries=10`, keyword-
  filter against intent (discard zero-overlap entries).
- **Step 3**: Derive structured constraint text: `MUST NOT` for procedural entries
  (failed approaches) and `SHOULD PREFER` for episodic entries (successful approaches).

The constraint block is injected with explicit headings:
```
--- MEMORY CONSTRAINTS (code-derived) ---
MUST NOT (prior failures ‚Äî do not repeat these approaches):
  - [tags: file, search] "used shell find -name *.go"
--- END CONSTRAINTS ---
```
`MUST NOT` framing gives the LLM a hard structural signal that the model respects.
No extra LLM call is needed for calibration; all logic is deterministic Go code.

**Added**
- `calibrate(entries []types.MemoryEntry, intent string) string` ‚Äî Steps 1‚Äì3
- `entrySummary(e types.MemoryEntry) string` ‚Äî human-readable entry line
- `memTokenize(s string) []string` ‚Äî keyword tokeniser (len‚â•3, lowercase)
- `maxMemoryEntries = 10` constant
- Tests: `internal/roles/planner/planner_test.go` ‚Äî one test per documented expectation
- Expectation comments on all three functions

---

## Issue #34 ‚Äî Validator criteria invisible in pipeline display

**Symptom**
The pipeline visualiser showed `MsgSubTask: #1 locate the audio file` with no hint
of what the subtask was being validated against. `MsgSubTaskOutcome` showed only
`matched` or `failed` with no detail about which criterion caused a failure.
Debugging validator behaviour required reading raw debug logs.

**Root cause**
`msgDetail()` in `display.go` rendered `MsgSubTask` as `#N intent` only (no
criteria). `MsgSubTaskOutcome` returned the bare `Status` string, discarding the
`GapTrajectory` and `UnmetCriteria` fields that are populated on failure.

**Fix**
Updated `msgDetail()`:

- **MsgSubTask**: appends `| <first criterion>` when `SuccessCriteria` is non-empty;
  appends `(+N)` suffix when there are multiple criteria. Example:
  `#1 locate audio | output contains a valid file path (+1)`

- **MsgSubTaskOutcome**: when `status="failed"` and `GapTrajectory` is non-empty,
  extracts the last trajectory point and shows the first unmet criterion:
  `failed | unmet: output contains a valid file path`
  When status is `matched` or no trajectory exists, returns the status string only.

**Added**
- Tests: `internal/ui/display_test.go` ‚Äî one test per documented expectation
- Expectation comments on `msgDetail()`
